data(mtcars)
best_lambda(formula = mpg ~ drat + wt + qsec,
data = mtcars, nfold = 3,
lambda_list = 10^seq(-2, 2, by = .1))
devtools::use_vignette("my-vignette")
library(devtools)
devtools::use_vignette("my-vignette")
devtools::build_vignettes()
library(usethis)
usethis::use_vignette("my-vignette")
usethis::use_vignette("homework-2")
library(bis557)
n <- 1000
p <- 25
beta <- c(1, rep(0, p - 1))
X <- matrix(rnorm(n * p), ncol = p)
X
beta
n <- 1000
p <- 25
beta <- c(1, rep(0, p - 1))
set.seed(2019)
X <- matrix(rnorm(n * p), ncol = p)
svals <- svd(X)$d
max(svals) / min(svals)
N <- 1e4
l2_errors <- rep(0, N)
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- casl_ols_svd(X, y)
l2_errors[k] <- sqrt(sum((betahat - beta)^2))
}
library(casl)
N <- 1e4
l2_errors <- rep(0, N)
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- casl_ols_svd(X, y)
l2_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(l2_errors)
n <- 1000
p <- 25
# Create a numerically stable X
set.seed(2019)
X <- matrix(rnorm(n * p), ncol = p)
# Set the first coordinate of beta to 1
beta <- c(1, rep(0, p - 1))
# Calculate condition number of X
svals <- svd(X)$d
max(svals) / min(svals)
n <- 1000
p <- 25
# Create a numerically stable X
set.seed(2019)
X <- matrix(rnorm(n * p), ncol = p)
# Set the first coordinate of beta to 1
beta <- c(1, rep(0, p - 1))
# Calculate condition number of X
svals <- svd(X)$d
max(svals) / min(svals)
# Do this 10,000 times
N <- 1e4
l2_errors <- rep(0, N)
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- casl_ols_svd(X, y)
l2_errors[k] <- sqrt(sum((betahat - beta)^2))
}
# Report the mean error rate
mean(l2_errors)
alpha <- 0.001
X[,1] <- X[,1] * alpha + X[,2] * (1 - alpha)
svals <- svd(X)$d max(svals) / min(svals)
alpha <- 0.001
X[,1] <- X[,1] * alpha + X[,2] * (1 - alpha)
svals <- svd(X)$d
max(svals) / min(svals)
N <- 1e4
l2_errors <- rep(0, N)
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- solve(crossprod(X), crossprod(X, y))
l2_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(l2_errors)
alpha <- 0.001
X[,1] <- X[,1] * alpha + X[,2] * (1 - alpha)
N <- 1e4
l2_errors <- rep(0, N)
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- solve(crossprod(X), crossprod(X, y))
l2_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(l2_errors)
alpha <- 0.001
X[,1] <- X[,1] * alpha + X[,2] * (1 - alpha)
N <- 1e4
l2_errors <- rep(0, N)
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- solve(crossprod(X), crossprod(X, y))
l2_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(l2_errors)
X
X
dim(X)
dim(beta)
length(beta)
y <- X %*% beta + rnorm(n)
y
data.frame(y, X)
data <- data.frame(y, X)
data
bestlambda <- best_lambda(formula = y ~ .,
data = data, nfold = 5,
lambda_list = 10^seq(-2, 2, by = .1))
bestlambda
ridge_regression(formula = y ~ ., data = data, lambda = bestlambda)$coefficients
betahat <- as.numeric(ridge_regression(formula = y ~ ., data = data, lambda = bestlambda)$coefficients)
sqrt(sum((betahat - beta)^2))
betahat
len(betahat)
length(betahat)
data(mtcars)
best_lambda(formula = mpg ~ drat + wt + qsec, data = mtcars, nfold = 3, lambda_list = 10^seq(-2, 2, by = .1))
ridge_regression(formula = Sepal.Length ~ ., data = iris, lambda = 1.8)
ridge_regression(formula = mpg ~ drat + wt + qsec, data = mtcars, lambda =  0.6309573)
data
model.matrix(y~., X)
model.matrix(y~., data = X)
model.matrix(y~., data = data)
model.matrix(y~., data = data)[ ,-1]
ridge_regression <- function(formula, data, lambda, intercept = TRUE) {
# Extract features and the label from the data
rownames(data) <- NULL
if (intercept == TRUE) {
X <- model.matrix(formula, data)
} else {
X <- model.matrix(formula, data)[ , -1]
}
# Ensure missing values are removed from both features and the label
y <- data[[as.character(formula)[2]]][as.numeric(rownames(X))]
# Use singular value decomposition to deal with colinear features
svd_obj <- svd(X)
U <- svd_obj$u
V <- svd_obj$v
svals <- svd_obj$d
D <- diag(svals / (svals^2 + lambda))
# Solve for ridge estimated coefficients
ridge_beta <- as.numeric(V %*% D %*% t(U) %*% y)
# Output results to a list
names(ridge_beta) <- colnames(X)
output <- list(coefficients = ridge_beta,
lambda = lambda,
formula = formula)
return(output)
}
betahat <- as.numeric(ridge_regression(formula = y ~ .,
data = data, intercept = FALSE,
lambda = bestlambda)$coefficients)
sqrt(sum((betahat - beta)^2))
best_lambda <- function(formula, data, nfold = 5, lambda_list,
intercept = TRUE) {
fsize <- floor(nrow(data) / nfold)
cv_mse <- rep(NA, length(lambda_list))
for (i in seq_along(lambda_list)) {
lambda <- lambda_list[i]
val_mse <- rep(NA, nfold)
if (nrow(data) %% fsize == 0) {
start_ind <- seq(from = 1, to = nrow(data), by = fsize )
end_ind <- seq(from = fsize, to = nrow(data), by = fsize)
} else if (nrow(data) %% fsize != 0) {
start_ind <- seq(from = 1, to = nrow(data), by = fsize )[1:nfold]
end_ind <- seq(from = fsize, to = nrow(data), by = fsize)
end_ind[nfold] <- nrow(data)
}
for (k in 1:nfold) {
rownames(data) <- NULL
val <- data[start_ind[k]:end_ind[k], ]
if (intercept == TRUE) {
val_X <- model.matrix(formula, val)
} else {
val_X <- model.matrix(formula, val)[ , -1]
}
val_y <- val[[as.character(formula)[2]]]
train <- data[setdiff(as.numeric(rownames(data)),
as.numeric(rownames(val_X))), ]
train_coef <- as.matrix(ridge_regression(formula, train, lambda,
intercept = intercept)$coefficients)
# Use ridge coefficients obtained from the training set
# to predict the label for the validation set
y_pred <- val_X %*% train_coef
val_mse[k] <- apply((y_pred - val_y)^2, 2, mean)
}
cv_mse[i] <- mean(val_mse)
}
return(lambda_list[cv_mse == min(cv_mse)])
}
best_lambda(formula = mpg ~ drat + wt + qsec,  data = mtcars, nfold = 3, lambda_list = 10^seq(-2, 2, by = .1), intercept = FALSE)
best_lambda(formula = mpg ~ drat + wt + qsec,  data = mtcars, nfold = 3, lambda_list = 10^seq(-2, 2, by = .1), intercept = TRUE)
N <- 10
l2_errors <- rep(0, N)
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
data <- data.frame(y, X)
bestlambda <- best_lambda(formula = y ~ .,
data = data, nfold = 5, intercept = FALSE,
lambda_list = 10^seq(-2, 2, by = .1))
betahat <- as.numeric(ridge_regression(formula = y ~ .,
data = data, intercept = FALSE,
lambda = bestlambda)$coefficients)
l2_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(l2_errors)
ridge_regression(formula = Sepal.Length ~ ., data = iris, lambda = 1.8)
X <- model.matrix(formula = Sepal.Length ~ ., data = iris)
X <- model.matrix(Sepal.Length ~ ., iris)
X
class(X)
iris$Sepal.Length
y <- as.matrix(iris$Sepal.Length)
casl_lm_ridge(X, y, lambda = 1.8)
ridge_regression(formula = Sepal.Length ~ ., data = iris, lambda = 1.8)
best_lambda(formula = Sepal.Length ~ ., data = iris, nfold = 5,lambda_list = 10^seq(-2, 2, by = .1))
ridge_regression(formula = Sepal.Length ~ ., data = iris, lambda = 0.13)
X <- model.matrix(Sepal.Length ~ ., iris)
y <- as.matrix(iris$Sepal.Length)
casl_lm_ridge(X, y, lambda = 0.13)
ridge_regression(formula = Sepal.Length ~ .,
data = iris, lambda = 0.13)$coefficients
as.numeric(ridge_regression(formula = Sepal.Length ~ .,
data = iris, lambda = 0.13)$coefficients)
myridge <- as.numeric(ridge_regression(formula = Sepal.Length ~ .,
data = iris, lambda = 0.13)$coefficients)
casl_lm_ridge(X, y, lambda = 0.13)
as.numeric(casl_lm_ridge(X, y, lambda = 0.13))
casl_ridge <-  as.numeric(casl_lm_ridge(X, y, lambda = 0.13))
expect_equivalent(my_ridge, casl_ridge, tolerance = 1e-5)
my_ridge
my_ridge <- as.numeric(ridge_regression(formula = Sepal.Length ~ .,
data = iris, lambda = 0.13)$coefficients)
X <- model.matrix(Sepal.Length ~ ., iris)
y <- as.matrix(iris$Sepal.Length)
casl_ridge <-  as.numeric(casl_lm_ridge(X, y, lambda = 0.13))
my_ridge
casl_ridge
usethis::use_testthat()
use_test()
use_test()
use_test()
use_test()
library(bis557)
library(testthat)
usethis::use_test("test-ridge")
devtools::test()
?ridge_regression
install.packages("reticulate")
library(reticulate)
#use_virtualenv("/")
use_python("/usr/local/bin/python")
X
X <- matrix(c(1, 5), nrow = 1)
X
svd_output <- svd(X)
X <- matrix(c(1, 5), nrow = 1)
X
svd_output <- svd(X)
U <- svd_output$u
V <- svd_output$v
U
V
svd
S <- svd_output$d
S
matrix(10)
X <- matrix(c(1, 5), nrow = 1)
y <- matrix(10)
svd_output <- svd(X)
U <- svd_output$u
V <- svd_output$v
S <- svd_output$d
beta <- V %*% (t(U) %*% y / S)
beta
t(U) %*% y
S
V
U
svd_output$d
svd_output$v
library(bis557)
library(casl)
n <- 1000
p <- 25
# Create a numerically stable X
set.seed(2019)
X <- matrix(rnorm(n * p), ncol = p)
# Set the first coordinate of beta to 1
beta <- c(1, rep(0, p - 1))
# Do this 100 times
N <- 100
l2_errors <- rep(0, N)
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- casl_ols_svd(X, y)
l2_errors[k] <- sqrt(sum((betahat - beta)^2))
}
# Report the mean error rate
mean(l2_errors)
alpha <- 0.001
X[,1] <- X[,1] * alpha + X[,2] * (1 - alpha)
N <- 100
l2_errors <- rep(0, N)
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- solve(crossprod(X), crossprod(X, y))
l2_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(l2_errors)
N <- 100
l2_errors <- rep(0, N)
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
data <- data.frame(y, X)
bestlambda <- best_lambda(formula = y ~ .,
data = data, nfold = 5, intercept = FALSE,
lambda_list = 10^seq(-2, 2, by = .1))
betahat <- as.numeric(ridge_regression(formula = y ~ .,
data = data, intercept = FALSE,
lambda = bestlambda)$coefficients)
l2_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(l2_errors)
X
ridge_error <- function(X) {
y <- X %*% beta + rnorm(n)
data <- data.frame(y, X)
bestlambda <- best_lambda(formula = y ~ .,
data = data, nfold = 5, intercept = FALSE,
lambda_list = 10^seq(-2, 2, by = .1))
betahat <- as.numeric(ridge_regression(formula = y ~ .,
data = data, intercept = FALSE,
lambda = bestlambda)$coefficients)
return(sqrt(sum((betahat - beta)^2)))
}
ridge_error(X = X)
ridge_error <- function(X) {
y <- X %*% beta + rnorm(n)
data <- data.frame(y, X)
bestlambda <- best_lambda(formula = y ~ .,
data = data, nfold = 5, intercept = FALSE,
lambda_list = 10^seq(-2, 2, by = .1))
betahat <- as.numeric(ridge_regression(formula = y ~ .,
data = data, intercept = FALSE,
lambda = bestlambda)$coefficients)
return(sqrt(sum((betahat - beta)^2)))
}
ridge_error(X = X)
ridge_error <- function(X) {
y <- X %*% beta + rnorm(n)
data <- data.frame(y, X)
bestlambda <- best_lambda(formula = y ~ .,
data = data, nfold = 5, intercept = FALSE,
lambda_list = 10^seq(-2, 2, by = .1))
betahat <- as.numeric(ridge_regression(formula = y ~ .,
data = data, intercept = FALSE,
lambda = bestlambda)$coefficients)
return(sqrt(sum((betahat - beta)^2)))
}
ridge_error(X = X)
replicate(10, ridge_error(X = X))
ridge_error <- function(X) {
y <- X %*% beta + rnorm(n)
data <- data.frame(y, X)
bestlambda <- best_lambda(formula = y ~ .,
data = data, nfold = 5, intercept = FALSE,
lambda_list = 10^seq(-2, 2, by = .1))
betahat <- as.numeric(ridge_regression(formula = y ~ .,
data = data, intercept = FALSE,
lambda = bestlambda)$coefficients)
return(sqrt(sum((betahat - beta)^2)))
}
mean(replicate(100, ridge_error(X = X)))
X
nrow(X)
X %*% beta + rnorm(nrow(X))
ols_error <- function(X, beta) {
# Randomly generate y based on X
y <- X %*% beta + rnorm(nrow(X))
# Use casl package to solve for betahat with svd
betahat <- casl_ols_svd(X, y)
# Calculate RMSE for betahat
return(sqrt(sum((betahat - beta)^2)))
}
ridge_error <- function(X, beta) {
# Randomly generate y based on X
y <- X %*% beta + rnorm(nrow(X))
data <- data.frame(y, X)
# Use my bis557 package to solve for betahat with ridge regression
# Use cross-validated best lambda to solve for betahat
bestlambda <- best_lambda(formula = y ~ .,
data = data, nfold = 5, intercept = FALSE,
lambda_list = 10^seq(-2, 2, by = .1))
betahat <- as.numeric(ridge_regression(formula = y ~ .,
data = data, intercept = FALSE,
lambda = bestlambda)$coefficients)
# Calculate RMSE for betahat
return(sqrt(sum((betahat - beta)^2)))
}
# Create a numerically stable X with 1000 rows and 25 columns
set.seed(2019)
X <- matrix(rnorm(1000 * 25), ncol = 25)
# Set the first coordinate of true beta to 1
beta <- c(1, rep(0, 25 - 1))
# Replicate the trial 100 times and report the average RMSE
mean(replicate(100, ols_error(X = X, beta = beta)))
# Replace the first column of X with a collinear column
# This creates a numerically unstable X
X[,1] <- X[,1] * 0.001 + X[,2] * (1 - 0.001)
# Replicate the OLS trial 100 times and report the average RMSE
mean(replicate(100, ols_error(X = X, beta = beta)))
179%%5
floor(179/5)
iris2 <- data(iris)
names(iris2)
head(iris2)
iris2
data(iris)
iris_c <- iris
head(iris_c)
iris_c[ ,3]
# Make Sepal.Width a linear combination of itself and Petal.Length
iris_c[ ,2] <- iris_c[ ,2] * 0.001 + iris_c[ ,3] * (1 - 0.001)
head(iris_c)
as.numeric(ridge_regression(formula = Sepal.Length ~ .,
data = iris_c,
lambda = 0.13)$coefficients)
iris_c <- iris
# Make Sepal.Width a linear combination of itself and Petal.Length
iris_c[ ,2] <- iris_c[ ,2] * 0.001 + iris_c[ ,3] * (1 - 0.001)
my_ridge <- as.numeric(ridge_regression(formula = Sepal.Length ~ .,
data = iris_c,
lambda = 0.13)$coefficients)
my_ridge
X <- model.matrix(Sepal.Length ~ ., iris_c)
y <- as.matrix(iris_c$Sepal.Length)
casl_ridge <-  as.numeric(casl_lm_ridge(X, y, lambda = 0.13))
expect_equivalent(my_ridge, casl_ridge, tolerance = 1e-5)
casl_ridge
best_lambda(formula = Sepal.Length ~ ., data = iris_c, lambda_list = 10^seq(-2, 2, by = .1))
my_ridge <- as.numeric(ridge_regression(formula = Sepal.Length ~ .,
data = iris_c,
lambda = 0.08)$coefficients)
X <- model.matrix(Sepal.Length ~ ., iris_c)
y <- as.matrix(iris_c$Sepal.Length)
casl_ridge <-  as.numeric(casl_lm_ridge(X, y, lambda = 0.08))
my_ridge
casl_ridge
library(bis557)
browseVignettes("packagename")
browseVignettes("bis557")
library(bis557)
browseVignettes("bis557")
browseVignettes("homework-2")
browseVignettes("bis557")
vignette(all = FALSE)
browseVignettes("dplyr")
browseVignettes("bis557")
devtools::build_vignettes()
browseVignettes("bis557")
library(bis557)
browseVignettes("bis557")
devtools::build_vignettes("bis557")
library(bis557)
library(casl)
# Create a numerically stable X with 1000 rows and 25 columns
set.seed(2019)
X <- matrix(rnorm(1000 * 25), ncol = 25)
svals <- svd(crossprod(X))$d
max(svals) / min(svals)
# Set the first coordinate of true beta to 1
beta <- c(1, rep(0, 25 - 1))
# Replicate the OLS trial 100 times and report the average RMSE
mean(replicate(100, ols_error(X = X, beta = beta)))
# Replace the first column of X with a collinear column
# This creates a numerically unstable X
X[ ,1] <- X[ ,1] * 0.001 + X[ ,2] * (1 - 0.001)
svals <- svd(crossprod(X))$d
max(svals) / min(svals)
library(bis557)
library(bis557)
?best_lambda
devtools::build_vignettes()
ridge_regression(formula = Sepal.Length ~ ., data = iris, lambda = 1.8)
library("bis557", lib.loc="~/Library/R/3.5/library")
browseVignettes("bis557")
library("bis557", lib.loc="~/Library/R/3.5/library")
ridge_regression(formula = Sepal.Length ~ ., data = iris, lambda = 1.8)
best_lambda(formula = Sepal.Length ~ .,  data = iris, nfold = 5, lambda_list = 10^seq(-2, 2, by = .1))
ridge_regression(formula = Sepal.Length ~ ., data = iris, lambda = 1.8)
library(bis557)
devtools::build_vignettes()
library(bis557)
library(casl)
# X is the same collinear design matrix from the last chunk
mean(replicate(25, ridge_error(X = X, beta = beta)))
best_lambda(formula = Sepal.Length ~ ., data = iris, nfold = 5, lambda_list = 10^seq(-2, 2, by = .1))
best_lambda(formula = mpg ~ drat + wt + qsec, data = mtcars, nfold = 3, lambda_list = 10^seq(-2, 2, by = .1))
ridge_regression(formula = Sepal.Length ~ ., data = iris, lambda = 1.8)
class(ridge_regression(formula = Sepal.Length ~ ., data = iris, lambda = 1.8))
